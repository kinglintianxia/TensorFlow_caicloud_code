{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 假设我们要最小化函数  $y=x^2$, 选择初始点   $x_0=5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 学习率为1的时候，x在5和-5之间震荡。\n",
    "$x_0-\\lambda*\\nabla f(x) = x_0-1*2*x_0 = -x_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 iteration(s): x1 is -5.000000.\n",
      "After 2 iteration(s): x2 is 5.000000.\n",
      "After 3 iteration(s): x3 is -5.000000.\n",
      "After 4 iteration(s): x4 is 5.000000.\n",
      "After 5 iteration(s): x5 is -5.000000.\n",
      "After 6 iteration(s): x6 is 5.000000.\n",
      "After 7 iteration(s): x7 is -5.000000.\n",
      "After 8 iteration(s): x8 is 5.000000.\n",
      "After 9 iteration(s): x9 is -5.000000.\n",
      "After 10 iteration(s): x10 is 5.000000.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "TRAINING_STEPS = 10\n",
    "LEARNING_RATE = 1\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # init all Variables.\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        x_value = sess.run(x)\n",
    "        print \"After %s iteration(s): x%s is %f.\"% (i+1, i+1, x_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 学习率为0.001的时候，下降速度过慢，在901轮时才收敛到0.823355。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 iteration(s): x1 is 4.990000.\n",
      "After 101 iteration(s): x101 is 4.084646.\n",
      "After 201 iteration(s): x201 is 3.343555.\n",
      "After 301 iteration(s): x301 is 2.736923.\n",
      "After 401 iteration(s): x401 is 2.240355.\n",
      "After 501 iteration(s): x501 is 1.833880.\n",
      "After 601 iteration(s): x601 is 1.501153.\n",
      "After 701 iteration(s): x701 is 1.228794.\n",
      "After 801 iteration(s): x801 is 1.005850.\n",
      "After 901 iteration(s): x901 is 0.823355.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_STEPS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        if i % 100 == 0: \n",
    "            x_value = sess.run(x)\n",
    "            print \"After %s iteration(s): x%s is %f.\"% (i+1, i+1, x_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. 使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得不错的收敛程度。\n",
    "```python\n",
    "tf.train.exponential_decay(\n",
    "    learning_rate, # starter learning rate\n",
    "    global_step, #\n",
    "    decay_steps, # 衰减速度\n",
    "    decay_rate, # 衰减系数\n",
    "    staircase=False, #  If True decay the learning rate at discrete intervals\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "And\n",
    "```python\n",
    "decayed_learning_rate = learning_rate *\n",
    "                        decay_rate ^ (global_step / decay_steps)\n",
    "```\n",
    "\n",
    "#### For this case:\n",
    "$\\text {decayed_learning_rate} = 0.1 *0.96 ^ {\\text {global_step} / 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 iteration(s): x1 is 4.000000, learning rate is 0.096000.\n",
      "After 11 iteration(s): x11 is 0.690561, learning rate is 0.063824.\n",
      "After 21 iteration(s): x21 is 0.222583, learning rate is 0.042432.\n",
      "After 31 iteration(s): x31 is 0.106405, learning rate is 0.028210.\n",
      "After 41 iteration(s): x41 is 0.065548, learning rate is 0.018755.\n",
      "After 51 iteration(s): x51 is 0.047625, learning rate is 0.012469.\n",
      "After 61 iteration(s): x61 is 0.038558, learning rate is 0.008290.\n",
      "After 71 iteration(s): x71 is 0.033523, learning rate is 0.005511.\n",
      "After 81 iteration(s): x81 is 0.030553, learning rate is 0.003664.\n",
      "After 91 iteration(s): x91 is 0.028727, learning rate is 0.002436.\n",
      "After 101 iteration(s): x101 is 0.027576, learning rate is 0.001620.\n",
      "After 111 iteration(s): x111 is 0.026837, learning rate is 0.001077.\n",
      "After 121 iteration(s): x121 is 0.026356, learning rate is 0.000716.\n",
      "After 131 iteration(s): x131 is 0.026042, learning rate is 0.000476.\n",
      "After 141 iteration(s): x141 is 0.025835, learning rate is 0.000316.\n",
      "After 151 iteration(s): x151 is 0.025698, learning rate is 0.000210.\n",
      "After 161 iteration(s): x161 is 0.025608, learning rate is 0.000140.\n",
      "After 171 iteration(s): x171 is 0.025548, learning rate is 0.000093.\n",
      "After 181 iteration(s): x181 is 0.025508, learning rate is 0.000062.\n",
      "After 191 iteration(s): x191 is 0.025482, learning rate is 0.000041.\n",
      "After 201 iteration(s): x201 is 0.025464, learning rate is 0.000027.\n",
      "After 211 iteration(s): x211 is 0.025452, learning rate is 0.000018.\n",
      "After 221 iteration(s): x221 is 0.025445, learning rate is 0.000012.\n",
      "After 231 iteration(s): x231 is 0.025439, learning rate is 0.000008.\n",
      "After 241 iteration(s): x241 is 0.025436, learning rate is 0.000005.\n",
      "After 251 iteration(s): x251 is 0.025434, learning rate is 0.000004.\n",
      "After 261 iteration(s): x261 is 0.025432, learning rate is 0.000002.\n",
      "After 271 iteration(s): x271 is 0.025431, learning rate is 0.000002.\n",
      "After 281 iteration(s): x281 is 0.025431, learning rate is 0.000001.\n",
      "After 291 iteration(s): x291 is 0.025430, learning rate is 0.000001.\n",
      "After 301 iteration(s): x301 is 0.025430, learning rate is 0.000000.\n",
      "After 311 iteration(s): x311 is 0.025430, learning rate is 0.000000.\n",
      "After 321 iteration(s): x321 is 0.025429, learning rate is 0.000000.\n",
      "After 331 iteration(s): x331 is 0.025429, learning rate is 0.000000.\n",
      "After 341 iteration(s): x341 is 0.025429, learning rate is 0.000000.\n",
      "After 351 iteration(s): x351 is 0.025429, learning rate is 0.000000.\n",
      "After 361 iteration(s): x361 is 0.025429, learning rate is 0.000000.\n",
      "After 371 iteration(s): x371 is 0.025429, learning rate is 0.000000.\n",
      "After 381 iteration(s): x381 is 0.025429, learning rate is 0.000000.\n",
      "After 391 iteration(s): x391 is 0.025429, learning rate is 0.000000.\n",
      "After 401 iteration(s): x401 is 0.025429, learning rate is 0.000000.\n",
      "After 411 iteration(s): x411 is 0.025429, learning rate is 0.000000.\n",
      "After 421 iteration(s): x421 is 0.025429, learning rate is 0.000000.\n",
      "After 431 iteration(s): x431 is 0.025429, learning rate is 0.000000.\n",
      "After 441 iteration(s): x441 is 0.025429, learning rate is 0.000000.\n",
      "After 451 iteration(s): x451 is 0.025429, learning rate is 0.000000.\n",
      "After 461 iteration(s): x461 is 0.025429, learning rate is 0.000000.\n",
      "After 471 iteration(s): x471 is 0.025429, learning rate is 0.000000.\n",
      "After 481 iteration(s): x481 is 0.025429, learning rate is 0.000000.\n",
      "After 491 iteration(s): x491 is 0.025429, learning rate is 0.000000.\n",
      "After 501 iteration(s): x501 is 0.025429, learning rate is 0.000000.\n",
      "After 511 iteration(s): x511 is 0.025429, learning rate is 0.000000.\n",
      "After 521 iteration(s): x521 is 0.025429, learning rate is 0.000000.\n",
      "After 531 iteration(s): x531 is 0.025429, learning rate is 0.000000.\n",
      "After 541 iteration(s): x541 is 0.025429, learning rate is 0.000000.\n",
      "After 551 iteration(s): x551 is 0.025429, learning rate is 0.000000.\n",
      "After 561 iteration(s): x561 is 0.025429, learning rate is 0.000000.\n",
      "After 571 iteration(s): x571 is 0.025429, learning rate is 0.000000.\n",
      "After 581 iteration(s): x581 is 0.025429, learning rate is 0.000000.\n",
      "After 591 iteration(s): x591 is 0.025429, learning rate is 0.000000.\n",
      "After 601 iteration(s): x601 is 0.025429, learning rate is 0.000000.\n",
      "After 611 iteration(s): x611 is 0.025429, learning rate is 0.000000.\n",
      "After 621 iteration(s): x621 is 0.025429, learning rate is 0.000000.\n",
      "After 631 iteration(s): x631 is 0.025429, learning rate is 0.000000.\n",
      "After 641 iteration(s): x641 is 0.025429, learning rate is 0.000000.\n",
      "After 651 iteration(s): x651 is 0.025429, learning rate is 0.000000.\n",
      "After 661 iteration(s): x661 is 0.025429, learning rate is 0.000000.\n",
      "After 671 iteration(s): x671 is 0.025429, learning rate is 0.000000.\n",
      "After 681 iteration(s): x681 is 0.025429, learning rate is 0.000000.\n",
      "After 691 iteration(s): x691 is 0.025429, learning rate is 0.000000.\n",
      "After 701 iteration(s): x701 is 0.025429, learning rate is 0.000000.\n",
      "After 711 iteration(s): x711 is 0.025429, learning rate is 0.000000.\n",
      "After 721 iteration(s): x721 is 0.025429, learning rate is 0.000000.\n",
      "After 731 iteration(s): x731 is 0.025429, learning rate is 0.000000.\n",
      "After 741 iteration(s): x741 is 0.025429, learning rate is 0.000000.\n",
      "After 751 iteration(s): x751 is 0.025429, learning rate is 0.000000.\n",
      "After 761 iteration(s): x761 is 0.025429, learning rate is 0.000000.\n",
      "After 771 iteration(s): x771 is 0.025429, learning rate is 0.000000.\n",
      "After 781 iteration(s): x781 is 0.025429, learning rate is 0.000000.\n",
      "After 791 iteration(s): x791 is 0.025429, learning rate is 0.000000.\n",
      "After 801 iteration(s): x801 is 0.025429, learning rate is 0.000000.\n",
      "After 811 iteration(s): x811 is 0.025429, learning rate is 0.000000.\n",
      "After 821 iteration(s): x821 is 0.025429, learning rate is 0.000000.\n",
      "After 831 iteration(s): x831 is 0.025429, learning rate is 0.000000.\n",
      "After 841 iteration(s): x841 is 0.025429, learning rate is 0.000000.\n",
      "After 851 iteration(s): x851 is 0.025429, learning rate is 0.000000.\n",
      "After 861 iteration(s): x861 is 0.025429, learning rate is 0.000000.\n",
      "After 871 iteration(s): x871 is 0.025429, learning rate is 0.000000.\n",
      "After 881 iteration(s): x881 is 0.025429, learning rate is 0.000000.\n",
      "After 891 iteration(s): x891 is 0.025429, learning rate is 0.000000.\n",
      "After 901 iteration(s): x901 is 0.025429, learning rate is 0.000000.\n",
      "After 911 iteration(s): x911 is 0.025429, learning rate is 0.000000.\n",
      "After 921 iteration(s): x921 is 0.025429, learning rate is 0.000000.\n",
      "After 931 iteration(s): x931 is 0.025429, learning rate is 0.000000.\n",
      "After 941 iteration(s): x941 is 0.025429, learning rate is 0.000000.\n",
      "After 951 iteration(s): x951 is 0.025429, learning rate is 0.000000.\n",
      "After 961 iteration(s): x961 is 0.025429, learning rate is 0.000000.\n",
      "After 971 iteration(s): x971 is 0.025429, learning rate is 0.000000.\n",
      "After 981 iteration(s): x981 is 0.025429, learning rate is 0.000000.\n",
      "After 991 iteration(s): x991 is 0.025429, learning rate is 0.000000.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_STEPS = 100\n",
    "global_step = tf.Variable(0)\n",
    "LEARNING_RATE = tf.train.exponential_decay(0.1, global_step, 1, 0.96, staircase=True)\n",
    "\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y, global_step=global_step)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        if i % 10 == 0:\n",
    "            LEARNING_RATE_value = sess.run(LEARNING_RATE)\n",
    "            x_value = sess.run(x)\n",
    "            print \"After %s iteration(s): x%s is %f, learning rate is %f.\"% (i+1, i+1, x_value, LEARNING_RATE_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
