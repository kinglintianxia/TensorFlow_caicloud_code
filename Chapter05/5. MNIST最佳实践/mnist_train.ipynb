{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# TensorFlow 提供了一个类来处理minist 数据集。自动下载并转化格式\n",
    "from tensorflow.examples.tutorials.mnist import input_data # mnist dataset operation\n",
    "import mnist_inference # mnist_inference.py\n",
    "# Common pathname manipulations. https://docs.python.org/3/library/os.path.html\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 定义神经网络结构相关的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30001\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"../MNIST_model/\"\n",
    "MODEL_NAME = \"mnist_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义训练过程。\n",
    "```python\n",
    "tf.train.exponential_decay(\n",
    "    learning_rate, # starter learning rate\n",
    "    global_step, #\n",
    "    decay_steps, # 衰减速度\n",
    "    decay_rate, # 衰减系数\n",
    "    staircase=False, #  If True decay the learning rate at discrete intervals\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "And\n",
    "$$decayed\\_learning\\_rate = learning\\_rate *decay\\_rate^{(global\\_step / decay\\_steps)}$$\n",
    "For this case:\n",
    "$decayed\\_learning\\_rate=0.8∗0.99^{global\\_step/mnist.train.num_examples / BATCH\\_SIZE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    # 定义输入输出placeholder。\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')\n",
    "    # weight regularizer\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    #调用mnist_inference.py 前向传播过程\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    # 滑动平均\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    # 计算交叉熵及其平均值\n",
    "    # tf.nn.softmax_cross_entropy_with_logits() = softmax + cross_entropy\n",
    "    # 在只有一个答案的分类问题中，只用`sparse_softmax_cross_entropy_with_logits`加速计算\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    # All loss\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    # 学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE,  # decay_steps, # 衰减速度\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    # train step\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 反向传播更新参数和更新每一个参数的滑动平均值\n",
    "    # 为了一次完成更新参数和滑动平均参数,tf提供了`tf.control_dependencies` 和`tf.group`两种机制。\n",
    "    # 等价于：train_op = tf.group(train_step, variables_averages_op)\n",
    "#     with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "#         train_op = tf.no_op(name='train')\n",
    "    train_op = tf.group(train_step, variables_averages_op)\n",
    "    \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # train steps.\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # Feed train data.\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 主程序入口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-5927980edbcf>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../0_datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../0_datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../0_datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../0_datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "After 1 training step(s), loss on training batch is 2.76234.\n",
      "After 1001 training step(s), loss on training batch is 0.242863.\n",
      "After 2001 training step(s), loss on training batch is 0.161668.\n",
      "After 3001 training step(s), loss on training batch is 0.141138.\n",
      "After 4001 training step(s), loss on training batch is 0.144606.\n",
      "After 5001 training step(s), loss on training batch is 0.114912.\n",
      "After 6001 training step(s), loss on training batch is 0.120412.\n",
      "After 7001 training step(s), loss on training batch is 0.101538.\n",
      "After 8001 training step(s), loss on training batch is 0.0791467.\n",
      "After 9001 training step(s), loss on training batch is 0.0699741.\n",
      "After 10001 training step(s), loss on training batch is 0.0666652.\n",
      "After 11001 training step(s), loss on training batch is 0.0674155.\n",
      "After 12001 training step(s), loss on training batch is 0.0570373.\n",
      "After 13001 training step(s), loss on training batch is 0.0624322.\n",
      "After 14001 training step(s), loss on training batch is 0.0515663.\n",
      "After 15001 training step(s), loss on training batch is 0.0499331.\n",
      "After 16001 training step(s), loss on training batch is 0.0536693.\n",
      "After 17001 training step(s), loss on training batch is 0.0505057.\n",
      "After 18001 training step(s), loss on training batch is 0.0465187.\n",
      "After 19001 training step(s), loss on training batch is 0.0459311.\n",
      "After 20001 training step(s), loss on training batch is 0.0381613.\n",
      "After 21001 training step(s), loss on training batch is 0.0414235.\n",
      "After 22001 training step(s), loss on training batch is 0.0411562.\n",
      "After 23001 training step(s), loss on training batch is 0.0354809.\n",
      "After 24001 training step(s), loss on training batch is 0.0368109.\n",
      "After 25001 training step(s), loss on training batch is 0.0337654.\n",
      "After 26001 training step(s), loss on training batch is 0.0420705.\n",
      "After 27001 training step(s), loss on training batch is 0.0367792.\n",
      "After 28001 training step(s), loss on training batch is 0.0319604.\n",
      "After 29001 training step(s), loss on training batch is 0.0317825.\n",
      "After 30001 training step(s), loss on training batch is 0.0356211.\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../../0_datasets/MNIST_data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
