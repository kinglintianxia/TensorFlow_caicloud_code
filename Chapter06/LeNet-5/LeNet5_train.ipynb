{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import LeNet5_inference # LeNet5_infernece.py\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1. 定义神经网络相关的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 60000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = \"../LeNet5_model/\"\n",
    "MODEL_NAME = \"lenet5_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义训练过程\n",
    "```python\n",
    "tf.placeholder(\n",
    "    dtype,\n",
    "    shape=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            LeNet5_inference.IMAGE_SIZE,\n",
    "            LeNet5_inference.IMAGE_SIZE,\n",
    "            LeNet5_inference.NUM_CHANNELS],\n",
    "            name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, LeNet5_inference.OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = LeNet5_inference.inference(x,True,regularizer) # train=False\n",
    "    # trainable: If True, the default, also adds the variable to the graph collection GraphKeys.\n",
    "    # TRAINABLE_VARIABLES. This collection is used as the default list of variables to use by the \n",
    "    # Optimizer classes.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    # loss\n",
    "    # tf.nn.softmax_cross_entropy_with_logits() = softmax + cross_entropy\n",
    "    # 在只有一个答案的分类问题中，只用`sparse_softmax_cross_entropy_with_logits`加速计算\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    # learning rate\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    # 等价于：train_op = tf.group(train_step, variables_averages_op)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    # Train\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                LeNet5_inference.IMAGE_SIZE,\n",
    "                LeNet5_inference.IMAGE_SIZE,\n",
    "                LeNet5_inference.NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 主程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-2511bcd4ff86>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../0_datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../0_datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../0_datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../0_datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "After 1000 training step(s), loss on training batch is 0.95644.\n",
      "After 2000 training step(s), loss on training batch is 0.69781.\n",
      "After 3000 training step(s), loss on training batch is 0.708384.\n",
      "After 4000 training step(s), loss on training batch is 0.803065.\n",
      "After 5000 training step(s), loss on training batch is 0.725942.\n",
      "After 6000 training step(s), loss on training batch is 0.668137.\n",
      "After 7000 training step(s), loss on training batch is 0.696918.\n",
      "After 8000 training step(s), loss on training batch is 0.730216.\n",
      "After 9000 training step(s), loss on training batch is 0.626685.\n",
      "After 10000 training step(s), loss on training batch is 0.620088.\n",
      "After 11000 training step(s), loss on training batch is 0.668061.\n",
      "After 12000 training step(s), loss on training batch is 0.661612.\n",
      "After 13000 training step(s), loss on training batch is 0.633575.\n",
      "After 14000 training step(s), loss on training batch is 0.706267.\n",
      "After 15000 training step(s), loss on training batch is 0.746101.\n",
      "After 16000 training step(s), loss on training batch is 0.635209.\n",
      "After 17000 training step(s), loss on training batch is 0.635836.\n",
      "After 18000 training step(s), loss on training batch is 0.655074.\n",
      "After 19000 training step(s), loss on training batch is 0.662251.\n",
      "After 20000 training step(s), loss on training batch is 0.628252.\n",
      "After 21000 training step(s), loss on training batch is 0.631541.\n",
      "After 22000 training step(s), loss on training batch is 0.609539.\n",
      "After 23000 training step(s), loss on training batch is 0.685601.\n",
      "After 24000 training step(s), loss on training batch is 0.626932.\n",
      "After 25000 training step(s), loss on training batch is 0.613716.\n",
      "After 26000 training step(s), loss on training batch is 0.612215.\n",
      "After 27000 training step(s), loss on training batch is 0.609888.\n",
      "After 28000 training step(s), loss on training batch is 0.637006.\n",
      "After 29000 training step(s), loss on training batch is 0.617178.\n",
      "After 30000 training step(s), loss on training batch is 0.59894.\n",
      "After 31000 training step(s), loss on training batch is 0.621207.\n",
      "After 32000 training step(s), loss on training batch is 0.610439.\n",
      "After 33000 training step(s), loss on training batch is 0.640226.\n",
      "After 34000 training step(s), loss on training batch is 0.637243.\n",
      "After 35000 training step(s), loss on training batch is 0.675182.\n",
      "After 36000 training step(s), loss on training batch is 0.661.\n",
      "After 37000 training step(s), loss on training batch is 0.640548.\n",
      "After 38000 training step(s), loss on training batch is 0.595192.\n",
      "After 39000 training step(s), loss on training batch is 0.672488.\n",
      "After 40000 training step(s), loss on training batch is 0.61407.\n",
      "After 41000 training step(s), loss on training batch is 0.59508.\n",
      "After 42000 training step(s), loss on training batch is 0.59651.\n",
      "After 43000 training step(s), loss on training batch is 0.595763.\n",
      "After 44000 training step(s), loss on training batch is 0.687511.\n",
      "After 45000 training step(s), loss on training batch is 0.589655.\n",
      "After 46000 training step(s), loss on training batch is 0.606913.\n",
      "After 47000 training step(s), loss on training batch is 0.615634.\n",
      "After 48000 training step(s), loss on training batch is 0.600247.\n",
      "After 49000 training step(s), loss on training batch is 0.612025.\n",
      "After 50000 training step(s), loss on training batch is 0.586786.\n",
      "After 51000 training step(s), loss on training batch is 0.608708.\n",
      "After 52000 training step(s), loss on training batch is 0.615826.\n",
      "After 53000 training step(s), loss on training batch is 0.657286.\n",
      "After 54000 training step(s), loss on training batch is 0.595285.\n",
      "After 55000 training step(s), loss on training batch is 0.626855.\n",
      "After 56000 training step(s), loss on training batch is 0.591433.\n",
      "After 57000 training step(s), loss on training batch is 0.605044.\n",
      "After 58000 training step(s), loss on training batch is 0.581249.\n",
      "After 59000 training step(s), loss on training batch is 0.591763.\n",
      "After 60000 training step(s), loss on training batch is 0.581589.\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../../0_datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
