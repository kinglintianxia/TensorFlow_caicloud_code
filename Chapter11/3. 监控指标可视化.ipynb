{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. 生成变量监控信息并定义生成监控信息日志的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_DIR = \"log/summaries.log\"\n",
    "BATCH_SIZE = 100\n",
    "TRAIN_STEPS = 3000\n",
    "# 生成变量监控信息\n",
    "# var:监控的张量，name:图标的名称\n",
    "def variable_summaries(var, name):\n",
    "    # 将生成监控信息的操作放到统一命名空间\n",
    "    with tf.name_scope('summaries'):\n",
    "        # tf.summary.histogram()记录张量中元素的信息分布，函数不会立即执行，\n",
    "        # 在调用sess.run()后才会真正生成并输出Summary portocol buffer.\n",
    "        tf.summary.histogram(name, var)\n",
    "        # 计算变量的平均值，日志标签名为:\"mean/name\"\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean/' + name, mean)\n",
    "        # 计算变量的标准差\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev/' + name, stddev)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 生成一层全链接的神经网络。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n",
    "            # call functions.\n",
    "            variable_summaries(weights, layer_name + '/weights')\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[output_dim]))\n",
    "            variable_summaries(biases, layer_name + '/biases')\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram(layer_name + '/pre_activations', preactivate)\n",
    "        # act = tf.nn.relu\n",
    "        activations = act(preactivate, name='activation')        \n",
    "        \n",
    "        # 记录神经网络节点输出在经过激活函数之后的分布。\n",
    "        tf.summary.histogram(layer_name + '/activations', activations)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"../0_datasets/MNIST_data/\", one_hot=True)\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "    hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "    y = nn_layer(hidden1, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    # tf.summary.histogram, tf.summary.image, tf.summary.scalar等函数需要调用sess.run(),一一调用非常麻烦，\n",
    "    # tf提供函数整理所有日志操作。\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(TRAIN_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            # 运行训练步骤以及所有的日志生成操作，得到这次运行的日志。\n",
    "            summary, loss, _ = sess.run([merged, cross_entropy, train_step], feed_dict={x: xs, y_: ys})\n",
    "            # 将得到的所有日志写入日志文件，这样TensorBoard程序就可以拿到这次运行所对应的\n",
    "            # 运行信息。\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            if i%100 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (i, loss))\n",
    "\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-7f1adc2b4ab6>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../0_datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../0_datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../0_datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../0_datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jun/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-4-7f1adc2b4ab6>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "After 0 training step(s), loss on training batch is 2.75048.\n",
      "After 100 training step(s), loss on training batch is 0.332166.\n",
      "After 200 training step(s), loss on training batch is 0.11869.\n",
      "After 300 training step(s), loss on training batch is 0.228527.\n",
      "After 400 training step(s), loss on training batch is 0.129583.\n",
      "After 500 training step(s), loss on training batch is 0.144685.\n",
      "After 600 training step(s), loss on training batch is 0.0743498.\n",
      "After 700 training step(s), loss on training batch is 0.0704668.\n",
      "After 800 training step(s), loss on training batch is 0.0559974.\n",
      "After 900 training step(s), loss on training batch is 0.0798436.\n",
      "After 1000 training step(s), loss on training batch is 0.0807357.\n",
      "After 1100 training step(s), loss on training batch is 0.141109.\n",
      "After 1200 training step(s), loss on training batch is 0.040944.\n",
      "After 1300 training step(s), loss on training batch is 0.0318172.\n",
      "After 1400 training step(s), loss on training batch is 0.244925.\n",
      "After 1500 training step(s), loss on training batch is 0.0429441.\n",
      "After 1600 training step(s), loss on training batch is 0.115018.\n",
      "After 1700 training step(s), loss on training batch is 0.0547584.\n",
      "After 1800 training step(s), loss on training batch is 0.0612027.\n",
      "After 1900 training step(s), loss on training batch is 0.0982544.\n",
      "After 2000 training step(s), loss on training batch is 0.025049.\n",
      "After 2100 training step(s), loss on training batch is 0.0693729.\n",
      "After 2200 training step(s), loss on training batch is 0.027017.\n",
      "After 2300 training step(s), loss on training batch is 0.036799.\n",
      "After 2400 training step(s), loss on training batch is 0.0355077.\n",
      "After 2500 training step(s), loss on training batch is 0.0661543.\n",
      "After 2600 training step(s), loss on training batch is 0.0862481.\n",
      "After 2700 training step(s), loss on training batch is 0.036571.\n",
      "After 2800 training step(s), loss on training batch is 0.0161113.\n",
      "After 2900 training step(s), loss on training batch is 0.0117945.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
